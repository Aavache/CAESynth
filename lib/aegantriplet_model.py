# External libs
import os
import torch
import itertools
from collections import OrderedDict
import torch
import torch.nn as nn
from torch.autograd import Variable
# Internal libs
from .base_model import BaseModel
from . import networks
from . import network_utils as net_utils
from . import loss

class AEGANTripletModel(BaseModel):
    """
    """
    def __init__(self, opt, is_train= True):
        """Initialize Autoencoder.

        Parameters:
            opt (dict)      - stores all the experiment flags; needs to be a subclass of BaseOptions
            is_train (bool) - Stage flag; {True: Training, False: Testing}
        """
        BaseModel.__init__(self, opt, is_train= True)

        # Instantiating networks
        self.AE = networks.instantiate_autoencoder(opt)
        self.AE.to(self.device)
        self.model_names = ['AE']

        if is_train:  # define discriminators
            # Specify the training losses you want to print out.
            self.loss_names = ['recon', 'pitch_trip', 'timbre_trip', 'g_gan', 'd_gan', 'gp']
            self.lambda_recon = opt['train']['lambda_recon']
            self.lambda_pitch_trp = opt['train']['lambda_pitch_trp']
            self.lambda_timbre_trp = opt['train']['lambda_timbre_trp']
            self.lambda_gan = opt['train']['lambda_gan']
            self.lambda_gp = opt['train']['lambda_gp']
            if self.lambda_recon == 0.0:
                self.loss_names.remove('recon')
            if self.lambda_gp == 0.0:
                self.loss_names.remove('gp')

            self.DISC = networks.instantiate_discriminator(opt)
            self.DISC.to(self.device)
            self.model_names += ['DISC']

            # Define loss functions
            if opt['train']['recon_mode'] == 'l1':
                self.criterion_recon = nn.L1Loss()
            else:
                self.criterion_recon = nn.MSELoss()
            self.criterion_triplet = loss.TripletLoss()
            self.criterion_gan = loss.GANLoss(opt['train']['gan_mode'], self.device)

            # Initialize optimizers
            self.optimizer_AE = torch.optim.Adam(self.AE.parameters(), lr=opt['train']['lr'], betas=(opt['train']['beta1'], 0.999))
            self.optimizer_E = torch.optim.Adam(self.AE.enc_net_params, lr=opt['train']['lr'], betas=(opt['train']['beta1'], 0.999))
            self.optimizer_DISC = torch.optim.Adam(self.DISC.parameters(), lr=opt['train']['lr'], betas=(opt['train']['beta1'], 0.999))

            if opt['train'].get('load', False):
                self.load_networks(opt['train'].get('load_suffix', 'latest'))
                print('Network Loaded!')
    
    def set_input(self, data):
        # Reference sample for both pitch and timbre
        self.anchor = data['anc_data'].to(self.device)
        # Positive pitch negative timbre sample
        self.dip1 = data['dip1_data'].to(self.device) 
        # Positive timbre negative pitch sample
        self.dip2 = data['dip2_data'].to(self.device) 

    def forward(self):
        """Run forward pass"""
        self.recon_anc,  self.z_anc = self.AE(self.anchor)
        self.recon_dip1,  self.z_dip1 = self.AE(self.dip1)
        self.recon_dip2,  self.z_dip2 = self.AE(self.dip2) 

    def generate(self, data):
        with torch.no_grad():
            recon,  z = self.AE(data)
            return recon, z

    def backward_DISC_basic(self, DISC, real, fake):
        """Calculate GAN loss for the discriminator

        Parameters:
            netD (network)      -- the discriminator D
            real (tensor array) -- real images
            fake (tensor array) -- images generated by a generator

        Return the discriminator loss.
        We also call loss_D.backward() to calculate the gradients.
        """
        # Real
        pred_real = DISC(real)
        loss_D_real = self.criterion_gan(pred_real, True)
        # Fake
        pred_fake = DISC(fake.detach())
        loss_D_fake = self.criterion_gan(pred_fake, False)
        # Gradient Penalty
        if self.lambda_gp > 0.0:
            real_data = real.type(torch.FloatTensor).to(self.device).data
            fake_data = fake.type(torch.FloatTensor).to(self.device).data
            loss_gp = loss.cal_gradient_penalty(DISC, real_data, fake_data, self.device, 
                                                lambda_gp= self.lambda_gp)[0]
        else:
            loss_gp = 0.0
        # Combined loss and calculate gradients
        loss_D = (loss_D_real + loss_D_fake) * 0.5 * self.lambda_gan
        loss_D_tot = loss_D + loss_gp

        loss_D_tot.backward(retain_graph=True)
        return loss_D, loss_gp

    def backward_DISC(self):
        """Calculate GAN loss for discriminator D_A"""
        loss_D_anc, loss_gp_anc = self.backward_DISC_basic(self.DISC, self.anchor, self.recon_anc)
        #loss_D_dip1, loss_gp_dip1 = self.backward_DISC_basic(self.DISC, self.dip1, self.recon_dip1)
        #loss_D_dip2, loss_gp_dip2 = self.backward_DISC_basic(self.DISC, self.dip2, self.recon_dip2)

        self.loss_d_gan = loss_D_anc #+ loss_D_dip1 + loss_D_dip2
        self.loss_gp = loss_gp_anc #+ loss_gp_dip1 + loss_gp_dip2

    def backward_AE(self):
        # Reconstruction loss
        self.loss_recon = self.criterion_recon(self.recon_anc, self.anchor)* self.lambda_recon
        self.loss_recon += self.criterion_recon(self.recon_dip1, self.dip1)* self.lambda_recon
        self.loss_recon += self.criterion_recon(self.recon_dip2, self.dip2)* self.lambda_recon

        # GAN loss
        self.loss_g_gan = self.criterion_gan(self.DISC(self.recon_anc), True)* self.lambda_gan 
        #self.loss_g_gan += self.criterion_gan(self.DISC(self.recon_dip1), True)* self.lambda_gan
        #self.loss_g_gan += self.criterion_gan(self.DISC(self.recon_dip2), True)* self.lambda_gan

        total_loss = self.loss_g_gan + self.loss_recon
        total_loss.backward(retain_graph=True)

    def backward_E(self):
        anc_tim, anc_pitch = torch.chunk(self.z_anc, chunks=2, dim=1)
        dip1_tim, dip1_pitch = torch.chunk(self.z_dip1, chunks=2, dim=1)
        dip2_tim, dip2_pitch = torch.chunk(self.z_dip2, chunks=2, dim=1)

        # Triple Pitch Loss
        self.loss_pitch_trip = self.criterion_triplet(anc_pitch, dip1_pitch, dip2_pitch)* self.lambda_pitch_trp
        # Triple Timbre Loss
        self.loss_timbre_trip = self.criterion_triplet(anc_tim, dip2_tim, dip1_tim)* self.lambda_timbre_trp

        total_loss = self.loss_timbre_trip + self.loss_pitch_trip
        total_loss.backward()

    def optimize_parameters(self):
        """Calculate losses, gradients, and update network weights; called in every training iteration"""
        # forward
        self.forward()

        net_utils.set_requires_grad([self.DISC], True)
        self.optimizer_DISC.zero_grad() 
        self.backward_DISC()  
        self.optimizer_DISC.step() 

        net_utils.set_requires_grad([self.DISC], False)
        self.optimizer_AE.zero_grad() 
        self.backward_AE()  
        self.optimizer_AE.step()

        self.optimizer_E.zero_grad() 
        self.backward_E()  
        self.optimizer_E.step()
